'''
Functions for creating Keras models from a configuration generated by Ray tune
-----
Kevin Nguyen
12/28/2020
'''
import sys
import configparser
import tensorflow as tf


def create_classifier_from_config(config, tupInputShape, metric=None, nMinNeurons=8):
    """Create a 1-output Keras classifier from a hyperparameter config generated by Tune

    Hyperparameters:

    Args:
        config (dict-like): config 
        tupInputShape (tuple): input shape (nFeatures, )
        metric: keras classification metric, defaults to tf.keras.metrics.AUC(curve='ROC', name='auroc')
        nMinNeurons (int): minimum number of neurons per hidden layer

    Returns:
        [model]: keras model
    """    
    if metric is None:
        metric = tf.keras.metrics.AUC(curve='ROC', name='auroc')

    layerIn = tf.keras.layers.Input(tupInputShape)
    x = layerIn

    for iLayer in range(config['hiddenlayers']):
        reg = tf.keras.regularizers.L1L2(l1=config['l1'], l2=config['l2'])            
        if iLayer == 0:
            # First hidden layer size is explicitly defined
            nNeurons = config['dense0_neurons']
        else:
            # Subsequent layers have size first_layer_size * (1 - taper_rate) ^ n_layer where taper_rate is a hyperparam
            nNeurons = config['dense0_neurons'] * ((1 - config['taper']) ** iLayer)
            nNeurons = int(nNeurons)
            if nNeurons < nMinNeurons:
                nNeurons = nMinNeurons

        x = tf.keras.layers.Dense(nNeurons, kernel_regularizer=reg)(x)
        # Batch norm, activation function, and dropout are the same for all hidden layers
        x = getattr(tf.keras.layers, config['activation'])()(x)
        if config['batchnorm']:
            x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(config['dropout'])(x)

    layerOut = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(layerIn, layerOut)
    opt = tf.keras.optimizers.Nadam(learning_rate=config['lr'], beta_1=config['beta_1'], beta_2=config['beta_2'])
    model.compile(loss='binary_crossentropy',
                    optimizer=opt,
                    metrics=[metric])

    return model

def create_regressor_from_config(config, tupInputShape):
    """Create a 1-output Keras regressor from a hyperparameter config generated by Tune

    Hyperparameters:

    Args:
        config (dict-like): config 
        tupInputShape (tuple): input shape (nFeatures, )

    Returns:
        [model]: Keras model
    """    
    layerIn = tf.keras.layers.Input(tupInputShape)
    x = layerIn

    for iLayer in range(config['hiddenlayers']):
        reg = tf.keras.regularizers.L1L2(l1=config['l1'], l2=config['l2'])            
        if iLayer == 0:
            # First hidden layer size is explicitly defined
            nNeurons = config['dense0_neurons']
        else:
            # Subsequent layers have size first_layer_size * (1 - taper_rate) ^ n_layer where taper_rate is a hyperparam
            nNeurons = config['dense0_neurons'] * ((1 - config['taper']) ** iLayer)
            nNeurons = int(nNeurons)
            # Minimum size is 16 neurons
            if nNeurons < 16:
                nNeurons = 16

        x = tf.keras.layers.Dense(nNeurons, kernel_regularizer=reg)(x)
        # Batch norm, activation function, and dropout are the same for all hidden layers
        x = getattr(tf.keras.layers, config['activation'])()(x)
        if config['batchnorm']:
            x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(config['dropout'])(x)

    layerOut = tf.keras.layers.Dense(1, activation='linear')(x)
    model = tf.keras.Model(layerIn, layerOut)
    opt = tf.keras.optimizers.Nadam(learning_rate=config['lr'], beta_1=config['beta_1'], beta_2=config['beta_2'])
    model.compile(loss='mean_squared_error',
                    optimizer=opt,
                    metrics=[rmse, rsquare])

    return model

def rmse(yTrue, yPred):
    """
    Custom loss function that computes RMSE
    :param yTrue:
    :param yPred:
    :return: RMSE
    """
    import tensorflow.keras.backend as K
    return K.sqrt(K.mean(K.pow(yTrue - yPred, 2)))


def rsquare(yTrue, yPred):
    """
    Returns the coefficient of determination R^2
    :param yTrue:
    :param yPred:
    :return:
    """
    import tensorflow.keras.backend as K
    ssRes = K.sum(K.square(yTrue - yPred))
    ssTotal = K.sum(K.square(yTrue - K.mean(yTrue)))
    return (1 - ssRes / (ssTotal + K.epsilon()))

